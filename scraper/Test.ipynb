{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de773ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_scraper import Company, actions\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6743df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "email = \"santlalc27@gmail.com\"\n",
    "password = \"Sant@123\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efcb279",
   "metadata": {},
   "source": [
    "### Service account for\n",
    "\n",
    "##### https://docs.google.com/spreadsheets/d/1vkbxhky9ek5JDZhyRpjD3Em8pxp0q9rSUh6Mfa5UKIA/edit?gid=0#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e3b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "SERVICE_ACCOUNT_FILE = \"/Users/abhay/Documents/Learn/Learning/Credentials/ordinal-quarter-387322-7194228669a8.json\"\n",
    "\n",
    "SCOPES = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "\n",
    "creds = Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "SHEET_NAME = \"HR Sheet\"\n",
    "\n",
    "sheet = client.open(SHEET_NAME).sheet1  # opens first sheet\n",
    "\n",
    "data = sheet.get_all_records()\n",
    "\n",
    "print(\"‚úÖ Successfully connected! Sample data:\")\n",
    "for row in data[:]:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e92ae",
   "metadata": {},
   "source": [
    "### Person data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c87c506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login timeout - verification likely needed\n",
      "Please complete any verification steps in the browser...\n",
      "Proceeding with profile scraping...\n",
      "‚úó Error during scraping: name 'Person' is not defined\n",
      "This might be due to LinkedIn's anti-scraping measures.\n",
      "Try the manual login approach or check if the profile is accessible.\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome()\n",
    "time.sleep(5)\n",
    "try:\n",
    "    actions.login(driver, email, password)\n",
    "    print(\"Login successful!\")\n",
    "except TimeoutException:\n",
    "    print(\"Login timeout - verification likely needed\")\n",
    "    print(\"Please complete any verification steps in the browser...\")\n",
    "    time.sleep(60)\n",
    "\n",
    "print(\"Proceeding with profile scraping...\")\n",
    "try:\n",
    "    # Try scraping with error handling\n",
    "    person = Person(\"https://www.linkedin.com/in/abhay-singh-patel-04b30b209/\", \n",
    "                   driver=driver, scrape=True)\n",
    "    print(\"‚úì Profile scraped successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPLETE CANDIDATE PROFILE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Basic Information\n",
    "    print(f\"\\nüîπ NAME: {person.name}\")\n",
    "    print(f\"üîπ PROFILE URL: {person.linkedin_url}\")\n",
    "    print(f\"üîπ CURRENT JOB TITLE: {person.job_title}\")\n",
    "    print(f\"üîπ CURRENT COMPANY: {person.company}\")\n",
    "    \n",
    "    # About Section\n",
    "    print(f\"\\nüìù ABOUT:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{person.about}\")\n",
    "    \n",
    "    # Work Experience\n",
    "    print(f\"\\nüíº WORK EXPERIENCE:\")\n",
    "    print(\"-\" * 40)\n",
    "    if person.experiences:\n",
    "        for i, exp in enumerate(person.experiences, 1):\n",
    "            print(f\"{i}. {exp.position_title} at {exp.institution_name}\")\n",
    "            if hasattr(exp, 'from_date') and exp.from_date:\n",
    "                print(f\"   üìÖ Duration: {exp.from_date} - {exp.to_date if exp.to_date else 'Present'}\")\n",
    "            if hasattr(exp, 'location') and exp.location:\n",
    "                print(f\"   üìç Location: {exp.location}\")\n",
    "            if hasattr(exp, 'description') and exp.description:\n",
    "                print(f\"   üìã Description: {exp.description}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No work experience information available\")\n",
    "    \n",
    "    # Education\n",
    "    print(f\"\\nüéì EDUCATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    if person.educations:\n",
    "        for i, edu in enumerate(person.educations, 1):\n",
    "            print(f\"{i}. {edu.degree} at {edu.institution_name}\")\n",
    "            if hasattr(edu, 'from_date') and edu.from_date:\n",
    "                print(f\"   üìÖ Duration: {edu.from_date} - {edu.to_date if edu.to_date else 'Present'}\")\n",
    "            if hasattr(edu, 'description') and edu.description:\n",
    "                print(f\"   üìã Description: {edu.description}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No education information available\")\n",
    "    \n",
    "    # Interests\n",
    "    print(f\"\\nüéØ INTERESTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    if person.interests:\n",
    "        for i, interest in enumerate(person.interests, 1):\n",
    "            print(f\"{i}. {interest.title}\")\n",
    "    else:\n",
    "        print(\"No interests information available\")\n",
    "    \n",
    "    # Accomplishments\n",
    "    print(f\"\\nüèÜ ACCOMPLISHMENTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    if person.accomplishments:\n",
    "        for i, acc in enumerate(person.accomplishments, 1):\n",
    "            print(f\"{i}. {acc.title}\")\n",
    "            if hasattr(acc, 'description') and acc.description:\n",
    "                print(f\"   üìã {acc.description}\")\n",
    "    else:\n",
    "        print(\"No accomplishments information available\")\n",
    "    \n",
    "    # Additional Debug Information\n",
    "    print(f\"\\nüîç DEBUG INFO:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Total Experiences: {len(person.experiences) if person.experiences else 0}\")\n",
    "    print(f\"Total Education entries: {len(person.educations) if person.educations else 0}\")\n",
    "    print(f\"Total Interests: {len(person.interests) if person.interests else 0}\")\n",
    "    print(f\"Total Accomplishments: {len(person.accomplishments) if person.accomplishments else 0}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error during scraping: {e}\")\n",
    "    print(\"This might be due to LinkedIn's anti-scraping measures.\")\n",
    "    print(\"Try the manual login approach or check if the profile is accessible.\")\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d71e3c",
   "metadata": {},
   "source": [
    "### Company data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "time.sleep(5)\n",
    "try:\n",
    "    actions.login(driver, email, password)\n",
    "except TimeoutException:\n",
    "    print(\"Login timeout - verification likely needed\")\n",
    "    print(\"Please complete any verification steps in the browser...\")\n",
    "    time.sleep(60)  # 1 minute to handle verification\n",
    "\n",
    "print(\"Proceeding with Google company scraping...\")\n",
    "try:\n",
    "    # Disable employee scraping to avoid timeout\n",
    "    company = Company(\"https://www.linkedin.com/company/google/\", driver=driver, get_employees=False)\n",
    "    \n",
    "    print(\"Company Information:\")\n",
    "    print(f\"Name: {company.name}\")\n",
    "    print(f\"About: {company.about_us}\")\n",
    "    print(f\"Website: {company.website}\")\n",
    "    print(f\"Headquarters: {company.headquarters}\")\n",
    "    print(f\"Founded: {company.founded}\")\n",
    "    print(f\"Company Type: {company.company_type}\")\n",
    "    print(f\"Company Size: {company.company_size}\")\n",
    "    print(f\"Specialties: {company.specialties}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error scraping company: {e}\")\n",
    "    print(\"This might be due to LinkedIn's anti-scraping measures or network issues.\")\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e210d678",
   "metadata": {},
   "source": [
    "### JOB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_scraper import JobSearch, actions\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Wait for page to load before attempting login\n",
    "time.sleep(5)  \n",
    "\n",
    "email = \"santlalc27@gmail.com\"\n",
    "password = \"Sant@123\"\n",
    "\n",
    "try:\n",
    "    actions.login(driver, email, password)\n",
    "except TimeoutException:\n",
    "    print(\"Login timeout - verification likely needed\")\n",
    "    print(\"Please complete any verification steps in the browser...\")\n",
    "    time.sleep(30)  # 30 seconds to handle verification\n",
    "\n",
    "print(\"Proceeding with job search scraping...\")\n",
    "\n",
    "try:\n",
    "    # Define multiple job keywords to search\n",
    "    job_keywords = [\n",
    "        \"software developer\",\n",
    "        # \"web developer\", \n",
    "        # \"python developer\",\n",
    "        # \"frontend developer\",\n",
    "        # \"backend developer\",\n",
    "        # \"full stack developer\",\n",
    "        # \"data scientist\",\n",
    "        # \"machine learning engineer\"\n",
    "    ]\n",
    "    \n",
    "    all_jobs_data = []\n",
    "    total_jobs_found = 0\n",
    "    seen_job_urls = set()  # To avoid duplicates\n",
    "    \n",
    "    print(f\"üöÄ Starting multi-job search for {len(job_keywords)} different job types...\")\n",
    "    \n",
    "    # Loop through each job keyword\n",
    "    for job_idx, job_keyword in enumerate(job_keywords, 1):\n",
    "        print(f\"\\nüîç [{job_idx}/{len(job_keywords)}] Searching for: '{job_keyword}'\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        keyword_jobs_count = 0\n",
    "        \n",
    "        # Loop through first 5 pages for each job keyword\n",
    "        for page in range(5):\n",
    "            start_param = page * 25\n",
    "            # URL encode the job keyword\n",
    "            encoded_keyword = job_keyword.replace(\" \", \"%20\")\n",
    "            job_search_url = f\"https://www.linkedin.com/jobs/search-results/?f_TPR=r86400&keywords={encoded_keyword}&start={start_param}\"\n",
    "            \n",
    "            print(f\"  üìÑ Page {page + 1}/5 - {job_keyword}\")\n",
    "            \n",
    "            driver.get(job_search_url)\n",
    "            time.sleep(8)  # Wait for page to load\n",
    "            \n",
    "            # Scroll to load more jobs on the page\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            scroll_attempts = 0\n",
    "            max_scroll_attempts = 2  # Reduced for efficiency\n",
    "            \n",
    "            while scroll_attempts < max_scroll_attempts:\n",
    "                # Scroll down to bottom\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(2)\n",
    "                \n",
    "                # Calculate new scroll height and compare with last scroll height\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    scroll_attempts += 1\n",
    "                else:\n",
    "                    scroll_attempts = 0  # Reset if we found new content\n",
    "                last_height = new_height\n",
    "            \n",
    "            # Get job cards after scrolling\n",
    "            try:\n",
    "                job_cards = WebDriverWait(driver, 15).until(\n",
    "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"[data-job-id]\"))\n",
    "                )\n",
    "                \n",
    "                page_jobs = []\n",
    "                \n",
    "                for i, job_card in enumerate(job_cards, 1):\n",
    "                    try:\n",
    "                        # Try multiple possible selectors for job title\n",
    "                        job_title = \"N/A\"\n",
    "                        job_url = \"N/A\"\n",
    "                        \n",
    "                        # Try various selectors for job title and link\n",
    "                        title_selectors = [\n",
    "                            \"h3 a span[title]\",\n",
    "                            \"h3 a\",\n",
    "                            \".base-search-card__title a\",\n",
    "                            \".job-card-container__link\",\n",
    "                            \"a[data-tracking-control-name*='job']\",\n",
    "                            \".base-card__full-link\",\n",
    "                            \"a span[aria-hidden='true']\"\n",
    "                        ]\n",
    "                        \n",
    "                        for selector in title_selectors:\n",
    "                            try:\n",
    "                                job_title_elem = job_card.find_element(By.CSS_SELECTOR, selector)\n",
    "                                if job_title_elem:\n",
    "                                    job_title = job_title_elem.text.strip() or job_title_elem.get_attribute(\"title\") or \"N/A\"\n",
    "                                    if job_title_elem.tag_name == \"a\":\n",
    "                                        job_url = job_title_elem.get_attribute(\"href\") or \"N/A\"\n",
    "                                    else:\n",
    "                                        # Find parent link\n",
    "                                        parent_link = job_title_elem.find_element(By.XPATH, \"./ancestor::a[1]\")\n",
    "                                        job_url = parent_link.get_attribute(\"href\") if parent_link else \"N/A\"\n",
    "                                    if job_title != \"N/A\":\n",
    "                                        break\n",
    "                            except:\n",
    "                                continue\n",
    "                        \n",
    "                        # Try to find company name with comprehensive selectors\n",
    "                        company_name = \"N/A\"\n",
    "                        company_selectors = [\n",
    "                            # Primary company selectors\n",
    "                            \".base-search-card__subtitle a\",\n",
    "                            \".job-search-card__subtitle-link\",\n",
    "                            \".job-card-container__company-name\",\n",
    "                            \"h4 a\",\n",
    "                            \n",
    "                            # Alternative company selectors\n",
    "                            \".base-search-card__subtitle\",\n",
    "                            \".job-search-card__subtitle\",\n",
    "                            \"a[data-tracking-control-name='public_jobs_jserp-result_job-search-card-subtitle']\",\n",
    "                            \"span[data-tracking-control-name='public_jobs_jserp-result_job-search-card-subtitle']\",\n",
    "                            \n",
    "                            # Generic selectors\n",
    "                            \"[data-tracking-control-name*='company']\",\n",
    "                            \".job-card-container__primary-description\",\n",
    "                            \"a[href*='/company/']\",\n",
    "                            \n",
    "                            # Fallback selectors\n",
    "                            \".artdeco-entity-lockup__subtitle\",\n",
    "                            \".job-result-card__subtitle\",\n",
    "                            \".jobs-search-results-list__subtitle\"\n",
    "                        ]\n",
    "                        \n",
    "                        for selector in company_selectors:\n",
    "                            try:\n",
    "                                company_elem = job_card.find_element(By.CSS_SELECTOR, selector)\n",
    "                                if company_elem:\n",
    "                                    # Try text first, then aria-label, then title attribute\n",
    "                                    company_text = (company_elem.text.strip() or \n",
    "                                                  company_elem.get_attribute(\"aria-label\") or \n",
    "                                                  company_elem.get_attribute(\"title\") or \"\").strip()\n",
    "                                    \n",
    "                                    # Clean up common prefixes/suffixes\n",
    "                                    if company_text and company_text not in [\"N/A\", \"\", \"Company\"]:\n",
    "                                        # Remove common prefixes like \"Company: \"\n",
    "                                        company_text = company_text.replace(\"Company:\", \"\").strip()\n",
    "                                        company_name = company_text\n",
    "                                        break\n",
    "                            except:\n",
    "                                continue\n",
    "                        \n",
    "                        # If still no company found, try finding it in nearby elements\n",
    "                        if company_name == \"N/A\":\n",
    "                            try:\n",
    "                                # Look for company info in sibling or parent elements\n",
    "                                all_text_elements = job_card.find_elements(By.CSS_SELECTOR, \"a, span, div\")\n",
    "                                for elem in all_text_elements[:10]:  # Check first 10 elements only\n",
    "                                    elem_text = elem.text.strip()\n",
    "                                    href = elem.get_attribute(\"href\") or \"\"\n",
    "                                    \n",
    "                                    # If element links to a company page, it's likely the company name\n",
    "                                    if \"/company/\" in href and elem_text and len(elem_text) < 100:\n",
    "                                        company_name = elem_text\n",
    "                                        break\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        # Try to find location with multiple selectors\n",
    "                        location = \"N/A\"\n",
    "                        location_selectors = [\n",
    "                            \".job-search-card__location\",\n",
    "                            \".base-search-card__metadata .job-card-container__metadata-item\",\n",
    "                            \".base-search-card__metadata\",\n",
    "                            \"[data-tracking-control-name*='location']\"\n",
    "                        ]\n",
    "                        \n",
    "                        for selector in location_selectors:\n",
    "                            try:\n",
    "                                location_elem = job_card.find_element(By.CSS_SELECTOR, selector)\n",
    "                                if location_elem:\n",
    "                                    location = location_elem.text.strip()\n",
    "                                    if location:\n",
    "                                        break\n",
    "                            except:\n",
    "                                continue\n",
    "                        \n",
    "                        # Check for duplicates using URL\n",
    "                        if job_url != \"N/A\" and job_url not in seen_job_urls:\n",
    "                            seen_job_urls.add(job_url)\n",
    "                            \n",
    "                            # Store job data\n",
    "                            job_data = {\n",
    "                                'title': job_title,\n",
    "                                'company': company_name,\n",
    "                                'location': location,\n",
    "                                'url': job_url,\n",
    "                                'job_type': job_keyword,\n",
    "                                'page': page + 1\n",
    "                            }\n",
    "                            page_jobs.append(job_data)\n",
    "                        \n",
    "                    except Exception as job_error:\n",
    "                        continue  # Skip problematic jobs\n",
    "                \n",
    "                all_jobs_data.extend(page_jobs)\n",
    "                keyword_jobs_count += len(page_jobs)\n",
    "                \n",
    "            except Exception as page_error:\n",
    "                print(f\"    ‚ùå Error on page {page + 1}: {str(page_error)[:50]}...\")\n",
    "                continue\n",
    "            \n",
    "            # Small delay between pages\n",
    "            time.sleep(2)\n",
    "        \n",
    "        print(f\"  ‚úÖ Found {keyword_jobs_count} unique jobs for '{job_keyword}'\")\n",
    "        total_jobs_found += keyword_jobs_count\n",
    "        \n",
    "        # Delay between different job searches\n",
    "        time.sleep(3)\n",
    "    \n",
    "    # Display all collected jobs organized by job type\n",
    "    print(f\"\\nüéØ FINAL RESULTS: {total_jobs_found} UNIQUE JOBS FROM {len(job_keywords)} JOB TYPES\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Group jobs by job type for better organization\n",
    "    jobs_by_type = {}\n",
    "    for job in all_jobs_data:\n",
    "        job_type = job['job_type']\n",
    "        if job_type not in jobs_by_type:\n",
    "            jobs_by_type[job_type] = []\n",
    "        jobs_by_type[job_type].append(job)\n",
    "    \n",
    "    # Display summary by job type\n",
    "    print(f\"\\nüìä SUMMARY BY JOB TYPE:\")\n",
    "    for job_type, jobs in jobs_by_type.items():\n",
    "        print(f\"  ‚Ä¢ {job_type}: {len(jobs)} jobs\")\n",
    "    \n",
    "    # Display all jobs\n",
    "    job_counter = 1\n",
    "    for job_type, jobs in jobs_by_type.items():\n",
    "        print(f\"\\n\\nüîπ {job_type.upper()} ({len(jobs)} jobs)\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for job in jobs:\n",
    "            print(f\"{job_counter}. {job['title']}\")\n",
    "            print(f\"    Company: {job['company']}\")\n",
    "            print(f\"    Location: {job['location']}\")\n",
    "            print(f\"    LinkedIn URL: {job['url']}\" if len(job['url']) > 80 else f\"    LinkedIn URL: {job['url']}\")\n",
    "            print()\n",
    "            job_counter += 1\n",
    "    \n",
    "    print(f\"üéâ Successfully scraped {total_jobs_found} unique job listings from {len(job_keywords)} job types!\")\n",
    "    print(f\"üìä Jobs were collected from up to 5 pages per job type.\")\n",
    "    print(f\"üö´ Duplicates removed: {len(seen_job_urls) - total_jobs_found} duplicate URLs filtered out.\")\n",
    "        \n",
    "except TimeoutException as te:\n",
    "    print(f\"Timeout error: Could not load job search page\")\n",
    "    print(\"This might be due to LinkedIn's anti-scraping measures or slow page loading.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during job search: {e}\")\n",
    "    print(\"LinkedIn may have updated their page structure or implemented new anti-scraping measures.\")\n",
    "    \n",
    "finally:\n",
    "    # Always close the driver\n",
    "    try:\n",
    "        input(\"Press Enter to close the browser...\")  # Keep browser open to see results\n",
    "        driver.quit()\n",
    "        print(\"Browser closed successfully.\")\n",
    "    except:\n",
    "        print(\"Browser was already closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1eb81",
   "metadata": {},
   "source": [
    "### Jobs V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Wait for page to load before attempting login\n",
    "time.sleep(5)  \n",
    "\n",
    "# email = \"santlalc27@gmail.com\"\n",
    "# password = \"Sant@123\"\n",
    "\n",
    "try:\n",
    "    actions.login(driver, email, password)\n",
    "except TimeoutException:\n",
    "    print(\"Login timeout - verification likely needed\")\n",
    "    print(\"Please complete any verification steps in the browser...\")\n",
    "    time.sleep(30)  # 30 seconds to handle verification\n",
    "\n",
    "print(\"Proceeding with job search scraping...\")\n",
    "\n",
    "try:\n",
    "    # Define multiple job keywords to search\n",
    "    job_keywords = [\n",
    "        \"software developer\",\n",
    "        # \"web developer\", \n",
    "        # \"python developer\",\n",
    "        # \"frontend developer\",\n",
    "        # \"backend developer\",\n",
    "        # \"full stack developer\",\n",
    "        # \"data scientist\",\n",
    "        # \"machine learning engineer\"\n",
    "    ]\n",
    "    \n",
    "    all_jobs_data = []\n",
    "    total_jobs_found = 0\n",
    "    seen_job_urls = set()  # To avoid duplicates\n",
    "    \n",
    "    print(f\"üöÄ Starting multi-job search for {len(job_keywords)} different job types...\")\n",
    "    \n",
    "    # Loop through each job keyword\n",
    "    for job_idx, job_keyword in enumerate(job_keywords, 1):\n",
    "        print(f\"\\nüîç [{job_idx}/{len(job_keywords)}] Searching for: '{job_keyword}'\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        keyword_jobs_count = 0\n",
    "        \n",
    "        # Loop through first 5 pages for each job keyword\n",
    "        for page in range(5):\n",
    "            start_param = page * 25\n",
    "            # URL encode the job keyword\n",
    "            encoded_keyword = job_keyword.replace(\" \", \"%20\")\n",
    "            job_search_url = f\"https://www.linkedin.com/jobs/search-results/?f_TPR=r86400&keywords={encoded_keyword}&start={start_param}\"\n",
    "            \n",
    "            print(f\"  üìÑ Page {page + 1}/5 - {job_keyword}\")\n",
    "            \n",
    "            driver.get(job_search_url)\n",
    "            time.sleep(8)  # Wait for page to load\n",
    "            \n",
    "            # Scroll to load more jobs on the page\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            scroll_attempts = 0\n",
    "            max_scroll_attempts = 2  # Reduced for efficiency\n",
    "            \n",
    "            while scroll_attempts < max_scroll_attempts:\n",
    "                # Scroll down to bottom\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(2)\n",
    "                \n",
    "                # Calculate new scroll height and compare with last scroll height\n",
    "                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                if new_height == last_height:\n",
    "                    scroll_attempts += 1\n",
    "                else:\n",
    "                    scroll_attempts = 0  # Reset if we found new content\n",
    "                last_height = new_height\n",
    "            \n",
    "            # Get job cards after scrolling\n",
    "            try:\n",
    "                job_cards = WebDriverWait(driver, 15).until(\n",
    "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"[data-job-id]\"))\n",
    "                )\n",
    "                \n",
    "                page_jobs = []\n",
    "                \n",
    "                for i, job_card in enumerate(job_cards, 1):\n",
    "                    try:\n",
    "                        # Try multiple possible selectors for job title\n",
    "                        job_title = \"N/A\"\n",
    "                        job_url = \"N/A\"\n",
    "                        \n",
    "                        # Try various selectors for job title and link\n",
    "                        title_selectors = [\n",
    "                            \"h3 a span[title]\",\n",
    "                            \"h3 a\",\n",
    "                            \".base-search-card__title a\",\n",
    "                            \".job-card-container__link\",\n",
    "                            \"a[data-tracking-control-name*='job']\",\n",
    "                            \".base-card__full-link\",\n",
    "                            \"a span[aria-hidden='true']\"\n",
    "                        ]\n",
    "                        \n",
    "                        for selector in title_selectors:\n",
    "                            try:\n",
    "                                job_title_elem = job_card.find_element(By.CSS_SELECTOR, selector)\n",
    "                                if job_title_elem:\n",
    "                                    job_title = job_title_elem.text.strip() or job_title_elem.get_attribute(\"title\") or \"N/A\"\n",
    "                                    if job_title_elem.tag_name == \"a\":\n",
    "                                        job_url = job_title_elem.get_attribute(\"href\") or \"N/A\"\n",
    "                                    else:\n",
    "                                        # Find parent link\n",
    "                                        parent_link = job_title_elem.find_element(By.XPATH, \"./ancestor::a[1]\")\n",
    "                                        job_url = parent_link.get_attribute(\"href\") if parent_link else \"N/A\"\n",
    "                                    if job_title != \"N/A\":\n",
    "                                        break\n",
    "                            except:\n",
    "                                continue\n",
    "                        \n",
    "                        # Try to find company name with comprehensive selectors\n",
    "                        company_name = \"N/A\"\n",
    "                        company_selectors = [\n",
    "                            # Primary company selectors\n",
    "                            \".base-search-card__subtitle a\",\n",
    "                            \".job-search-card__subtitle-link\",\n",
    "                            \".job-card-container__company-name\",\n",
    "                            \"h4 a\",\n",
    "                            \n",
    "                            # Alternative company selectors\n",
    "                            \".base-search-card__subtitle\",\n",
    "                            \".job-search-card__subtitle\",\n",
    "                            \"a[data-tracking-control-name='public_jobs_jserp-result_job-search-card-subtitle']\",\n",
    "                            \"span[data-tracking-control-name='public_jobs_jserp-result_job-search-card-subtitle']\",\n",
    "                            \n",
    "                            # Generic selectors\n",
    "                            \"[data-tracking-control-name*='company']\",\n",
    "                            \".job-card-container__primary-description\",\n",
    "                            \"a[href*='/company/']\",\n",
    "                            \n",
    "                            # Fallback selectors\n",
    "                            \".artdeco-entity-lockup__subtitle\",\n",
    "                            \".job-result-card__subtitle\",\n",
    "                            \".jobs-search-results-list__subtitle\"\n",
    "                        ]\n",
    "                        \n",
    "                        for selector in company_selectors:\n",
    "                            try:\n",
    "                                company_elem = job_card.find_element(By.CSS_SELECTOR, selector)\n",
    "                                if company_elem:\n",
    "                                    # Try text first, then aria-label, then title attribute\n",
    "                                    company_text = (company_elem.text.strip() or \n",
    "                                                  company_elem.get_attribute(\"aria-label\") or \n",
    "                                                  company_elem.get_attribute(\"title\") or \"\").strip()\n",
    "                                    \n",
    "                                    # Clean up common prefixes/suffixes\n",
    "                                    if company_text and company_text not in [\"N/A\", \"\", \"Company\"]:\n",
    "                                        # Remove common prefixes like \"Company: \"\n",
    "                                        company_text = company_text.replace(\"Company:\", \"\").strip()\n",
    "                                        company_name = company_text\n",
    "                                        break\n",
    "                            except:\n",
    "                                continue\n",
    "                        \n",
    "                        # If still no company found, try finding it in nearby elements\n",
    "                        if company_name == \"N/A\":\n",
    "                            try:\n",
    "                                # Look for company info in sibling or parent elements\n",
    "                                all_text_elements = job_card.find_elements(By.CSS_SELECTOR, \"a, span, div\")\n",
    "                                for elem in all_text_elements[:10]:  # Check first 10 elements only\n",
    "                                    elem_text = elem.text.strip()\n",
    "                                    href = elem.get_attribute(\"href\") or \"\"\n",
    "                                    \n",
    "                                    # If element links to a company page, it's likely the company name\n",
    "                                    if \"/company/\" in href and elem_text and len(elem_text) < 100:\n",
    "                                        company_name = elem_text\n",
    "                                        break\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        # Try to find location with multiple selectors\n",
    "                        location = \"N/A\"\n",
    "                        location_selectors = [\n",
    "                            \".job-search-card__location\",\n",
    "                            \".base-search-card__metadata .job-card-container__metadata-item\",\n",
    "                            \".base-search-card__metadata\",\n",
    "                            \"[data-tracking-control-name*='location']\"\n",
    "                        ]\n",
    "                        \n",
    "                        for selector in location_selectors:\n",
    "                            try:\n",
    "                                location_elem = job_card.find_element(By.CSS_SELECTOR, selector)\n",
    "                                if location_elem:\n",
    "                                    location = location_elem.text.strip()\n",
    "                                    if location:\n",
    "                                        break\n",
    "                            except:\n",
    "                                continue\n",
    "                        \n",
    "                        # Check for duplicates using URL\n",
    "                        if job_url != \"N/A\" and job_url not in seen_job_urls:\n",
    "                            seen_job_urls.add(job_url)\n",
    "                            \n",
    "                            # Store job data\n",
    "                            job_data = {\n",
    "                                'title': job_title,\n",
    "                                'company': company_name,\n",
    "                                'location': location,\n",
    "                                'url': job_url,\n",
    "                                'job_type': job_keyword,\n",
    "                                'page': page + 1\n",
    "                            }\n",
    "                            page_jobs.append(job_data)\n",
    "                        \n",
    "                    except Exception as job_error:\n",
    "                        continue  # Skip problematic jobs\n",
    "                \n",
    "                all_jobs_data.extend(page_jobs)\n",
    "                keyword_jobs_count += len(page_jobs)\n",
    "                \n",
    "            except Exception as page_error:\n",
    "                print(f\"    ‚ùå Error on page {page + 1}: {str(page_error)[:50]}...\")\n",
    "                continue\n",
    "            \n",
    "            # Small delay between pages\n",
    "            time.sleep(2)\n",
    "        \n",
    "        print(f\"  ‚úÖ Found {keyword_jobs_count} unique jobs for '{job_keyword}'\")\n",
    "        total_jobs_found += keyword_jobs_count\n",
    "        \n",
    "        # Delay between different job searches\n",
    "        time.sleep(3)\n",
    "    \n",
    "    # Display all collected jobs organized by job type\n",
    "    print(f\"\\nüéØ FINAL RESULTS: {total_jobs_found} UNIQUE JOBS FROM {len(job_keywords)} JOB TYPES\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Group jobs by job type for better organization\n",
    "    jobs_by_type = {}\n",
    "    for job in all_jobs_data:\n",
    "        job_type = job['job_type']\n",
    "        if job_type not in jobs_by_type:\n",
    "            jobs_by_type[job_type] = []\n",
    "        jobs_by_type[job_type].append(job)\n",
    "    \n",
    "    # Display summary by job type\n",
    "    print(f\"\\nüìä SUMMARY BY JOB TYPE:\")\n",
    "    for job_type, jobs in jobs_by_type.items():\n",
    "        print(f\"  ‚Ä¢ {job_type}: {len(jobs)} jobs\")\n",
    "    \n",
    "    # Display all jobs\n",
    "    job_counter = 1\n",
    "    for job_type, jobs in jobs_by_type.items():\n",
    "        print(f\"\\n\\nüîπ {job_type.upper()} ({len(jobs)} jobs)\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for job in jobs:\n",
    "            print(f\"{job_counter}. {job['title']}\")\n",
    "            print(f\"    Company: {job['company']}\")\n",
    "            print(f\"    Location: {job['location']}\")\n",
    "            print(f\"    LinkedIn URL: {job['url']}\" if len(job['url']) > 80 else f\"    LinkedIn URL: {job['url']}\")\n",
    "            print()\n",
    "            job_counter += 1\n",
    "    \n",
    "    print(f\"üéâ Successfully scraped {total_jobs_found} unique job listings from {len(job_keywords)} job types!\")\n",
    "    print(f\"üìä Jobs were collected from up to 5 pages per job type.\")\n",
    "    print(f\"üö´ Duplicates removed: {len(seen_job_urls) - total_jobs_found} duplicate URLs filtered out.\")\n",
    "        \n",
    "except TimeoutException as te:\n",
    "    print(f\"Timeout error: Could not load job search page\")\n",
    "    print(\"This might be due to LinkedIn's anti-scraping measures or slow page loading.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during job search: {e}\")\n",
    "    print(\"LinkedIn may have updated their page structure or implemented new anti-scraping measures.\")\n",
    "    \n",
    "finally:\n",
    "    # Always close the driver\n",
    "    try:\n",
    "        input(\"Press Enter to close the browser...\")  # Keep browser open to see results\n",
    "        driver.quit()\n",
    "        print(\"Browser closed successfully.\")\n",
    "    except:\n",
    "        print(\"Browser was already closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
